{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06/02/25 \n",
    "# Gabriella Chan\n",
    "# gabriella.chan@moansh.edu\n",
    "\n",
    "# This script generates a memory mapped distance matrix using the brainsmash volume function\n",
    "# It loads an atlas, takes coordinates of valid rois, and calls \"volume\" to generate the distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from brainsmash.workbench import volume\n",
    "from brainsmash.mapgen.eval import sampled_fit\n",
    "from brainsmash.mapgen.sampled import Sampled\n",
    "import os\n",
    "\n",
    "import time\n",
    "from ttictoc import tic, toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/l_cx/'\n",
    "out_dir = './results/l_cx/'\n",
    "\n",
    "# Load atlas and brain map\n",
    "atlas_img = nib.load('/fs03/kg98/gchan/Atlases/Tian/Schaefer_Tian/reordered/Schaefer2018_100Parcels_' +\n",
    "    '7Networks_order_Tian_Subcortex_S2_MNI152NLin6Asym_1.5mm_reordered.nii.gz')\n",
    "atlas = atlas_img.get_fdata()\n",
    "img = nib.load('./data/lme_betas.nii.gz')\n",
    "img_data = img.get_fdata()\n",
    "\n",
    "roi_i = 1\n",
    "roi_j = 50\n",
    "mask = (atlas >= roi_i) & (atlas <= roi_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading voxels coordinates from ./data/l_cx/coordinates.txt\n",
      "file contains 156579 voxels\n",
      "saving memory-mapped distance matrix files to ./data/l_cx/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2663.243753605522"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_distmat = True\n",
    "tic()\n",
    "if gen_distmat:\n",
    "    # Get the 3D coordinates and values of the masked image\n",
    "    coords = np.column_stack(np.where(mask))\n",
    "    coord_file = data_dir + 'coordinates.txt'\n",
    "    np.savetxt(coord_file, coords, fmt='%d')\n",
    "\n",
    "    gmv = img_data[np.where(mask)]\n",
    "    brain_map = data_dir + 'lme_betas.txt'\n",
    "    np.savetxt(brain_map, gmv, fmt='%f')\n",
    "\n",
    "    # volume is a builtin BrainSMASH function that generates a memory-mapped distance matrix\n",
    "    # This has a very long runtime, ~1 h for 200k voxels\n",
    "    filenames = volume(coord_file, data_dir)\n",
    "else:\n",
    "    brain_map = data_dir + 'lme_betas.txt'\n",
    "    filenames = {\n",
    "        \"D\": data_dir + \"distmat.npy\",\n",
    "        \"index\": data_dir + \"index.npy\"\n",
    "    }\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 min lh knn 1500, 14 min knn 2300, 13 tasks??\n",
    "# 30 min l cx, p4 6 tasks\n",
    "\n",
    "# These are three of the key parameters affecting the variogram fit\n",
    "kwargs = {\n",
    "    'ns': 500,\n",
    "    'knn': 1500,\n",
    "    'pv': 60,\n",
    "    'kernel': 'gaussian',\n",
    "    'n_jobs': 13\n",
    "}\n",
    "\n",
    "# Running this command will generate a matplotlib figure\n",
    "# tic()\n",
    "# sampled_fit(brain_map, filenames['D'], filenames['index'], nsurr=10, **kwargs)\n",
    "# toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.utils.validation import check_random_state\n",
    "\n",
    "# 20 min 200k voxels, 13 tasks\n",
    "\n",
    "try:\n",
    "    with open(data_dir + 'gen.pkl', 'rb') as f:\n",
    "        gen = pickle.load(f)\n",
    "        # reset random state in generator\n",
    "        gen._rs = check_random_state(None)\n",
    "except FileNotFoundError:\n",
    "    gen = Sampled(x=brain_map, D=filenames['D'], index=filenames['index'], **kwargs)\n",
    "    with open(data_dir + 'gen.pkl', 'wb') as f:\n",
    "        pickle.dump(gen, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for generator for l_cx\n",
      "Generator found for l_cx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7075775200501084"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.utils.validation import check_random_state\n",
    "tic()\n",
    "\n",
    "name = \"l_cx\"\n",
    "# 20 min 200k voxels, 13 tasks\n",
    "\n",
    "print(\"Searching for generator for \" + name)\n",
    "try:\n",
    "    with open(data_dir + 'gen.pkl', 'rb') as f:\n",
    "        gen = pickle.load(f)\n",
    "        # reset random state in generator\n",
    "        gen._rs = check_random_state(None)\n",
    "        print(\"Generator found for \" + name)\n",
    "except FileNotFoundError:\n",
    "    print(\"Generator not found. Creating generator for \" + name)\n",
    "    gen = Sampled(x=brain_map, D=filenames['D'], index=filenames['index'], **kwargs)\n",
    "    with open(data_dir + 'gen.pkl', 'wb') as f:\n",
    "        pickle.dump(gen, f)\n",
    "    print(\"Generator created for \" + name)\n",
    "\n",
    "\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156579,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.373763739131391"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is no economies of scale in generating multiple surrogate maps\n",
    "# 4h for 1 null map of 166k voxels\n",
    "\n",
    "tic()\n",
    "n_maps = 1\n",
    "surrogate_maps = gen(n=n_maps)\n",
    "print(surrogate_maps.shape)\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_map(null_map, out_dir, filename):\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    modified_img = nib.Nifti1Image(null_map, img.affine, img.header)\n",
    "    nib.save(modified_img, out_dir + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_map = np.zeros_like(img_data, dtype=float)\n",
    "if n_maps == 1:\n",
    "    null_map[mask] = surrogate_maps\n",
    "    save_map(null_map, out_dir, \"test2.nii.gz\")\n",
    "else:\n",
    "    for i in range(n_maps):\n",
    "        null_map[mask] = surrogate_maps[i]\n",
    "        save_map(null_map, out_dir, f\"null_map_{i}.nii.gz\")\n",
    "\n",
    "# np.savetxt(out_dir + 'mask.csv', mask, delimiter=',', fmt='%d')\n",
    "# np.savetxt(out_dir + 'surrogate_maps.csv', surrogate_maps, delimiter=',', fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
